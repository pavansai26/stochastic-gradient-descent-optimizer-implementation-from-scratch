{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stochastic gradient optimizer implementation from scratch.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyPQhzJuJgLgXdewBHAbXXVN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/stochastic-gradient-descent-optimizer-implementation-from-scratch/blob/master/stochastic_gradient_optimizer_implementation_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVZqsCNcnVJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PelCiiQgowyJ",
        "colab_type": "text"
      },
      "source": [
        "# Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeKvcEN9o4oB",
        "colab_type": "text"
      },
      "source": [
        "$$ \\mathbf{w}' = \\mathbf{w} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRXXxfj6o7Yg",
        "colab_type": "text"
      },
      "source": [
        "$$ \\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\eta_{t} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}_{t}} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDj9k220pHZS",
        "colab_type": "text"
      },
      "source": [
        "Learning rate $\\eta_{t}$ can be changed at time $t$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRQne3sppLlW",
        "colab_type": "text"
      },
      "source": [
        "importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrmWZBcKozZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffVdnMyzpVIA",
        "colab_type": "text"
      },
      "source": [
        "function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2bnxJYfpXp0",
        "colab_type": "text"
      },
      "source": [
        "$$ f(x, y) = (1.5 - x + xy)^{2} + (2.25 - x + xy^{2})^{2} + (2.625 - x +xy^{3})^{2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd0jjUuipeMv",
        "colab_type": "text"
      },
      "source": [
        "function implementation in python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5DUir51pSO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "f = lambda x, y: (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSlR0G1KppYj",
        "colab_type": "text"
      },
      "source": [
        "calculating the gradients of the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4CKbR5Ypln5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradients(x, y):\n",
        "  \"\"\"Gradient of Beale function.\n",
        "\n",
        "  Args:\n",
        "    x: x-dimension of inputs\n",
        "    y: y-dimension of inputs\n",
        "\n",
        "  Returns:\n",
        "    grads: [dx, dy], shape: 1-rank Tensor (vector) np.array\n",
        "      dx: gradient of Beale function with respect to x-dimension of inputs\n",
        "      dy: gradient of Beale function with respect to y-dimension of inputs\n",
        "  \"\"\"\n",
        "  dx = 2. * ( (1.5 - x + x * y) * (y - 1) + \\\n",
        "                (2.25 - x + x * y**2) * (y**2 - 1) + \\\n",
        "                (2.625 - x + x * y**3) * (y**3 - 1) )\n",
        "  dy = 2. * ( (1.5 - x + x * y) * x + \\\n",
        "              (2.25 - x + x * y**2) * 2. * x * y + \\\n",
        "              (2.625 - x + x * y**3) * 3. * x * y**2 )\n",
        "  grads = np.array([dx, dy])\n",
        "  return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKRKsphjp0s3",
        "colab_type": "text"
      },
      "source": [
        "building the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6prKE-npu-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GradientDescentOptimizer():\n",
        "  def __init__(self, function, gradients, x_init=None, y_init=None, learning_rate=0.01):\n",
        "    self.f = function\n",
        "    self.g = gradients\n",
        "    scale = 3.0\n",
        "    self.vars = np.zeros([2])\n",
        "\n",
        "    if x_init is not None:\n",
        "      self.vars[0] = x_init\n",
        "    else:\n",
        "      self.vars[0] = np.random.uniform(low=-scale, high=scale)\n",
        "\n",
        "    if y_init is not None:\n",
        "      self.vars[1] = y_init\n",
        "    else:\n",
        "      self.vars[1] = np.random.uniform(low=-scale, high=scale)\n",
        "\n",
        "    print(\"x_init: {:.3f}\".format(self.vars[0]))\n",
        "    print(\"y_init: {:.3f}\".format(self.vars[1]))\n",
        "\n",
        "    self.lr = learning_rate\n",
        "\n",
        "     # for accumulation of loss and path (w, b)\n",
        "    self.z_history = []\n",
        "    self.x_history = []\n",
        "    self.y_history = []\n",
        "  \n",
        "  def func(self, variables):\n",
        "    \"\"\"Beale function.\n",
        "    \n",
        "    Args:\n",
        "      variables: input data, shape: 1-rank Tensor (vector) np.array\n",
        "        x: x-dimension of inputs\n",
        "        y: y-dimension of inputs\n",
        "      \n",
        "    Returns:\n",
        "      z: Beale function value at (x, y)\n",
        "    \"\"\"\n",
        "    x, y = variables\n",
        "    z = self.f(x, y)\n",
        "    return z\n",
        "\n",
        "  def gradients(self, variables):\n",
        "    \"\"\"Gradient of Beale function.\n",
        "    \n",
        "    Args:\n",
        "      variables: input data, shape: 1-rank Tensor (vector) np.array\n",
        "        x: x-dimension of inputs\n",
        "        y: y-dimension of inputs\n",
        "      \n",
        "    Returns:\n",
        "      grads: [dx, dy], shape: 1-rank Tensor (vector) np.array\n",
        "        dx: gradient of Beale function with respect to x-dimension of inputs\n",
        "        dy: gradient of Beale function with respect to y-dimension of inputs\n",
        "    \"\"\"\n",
        "    x, y = variables\n",
        "    grads = self.g(x, y)\n",
        "    return grads\n",
        "  \n",
        "  def weights_update(self, grads):\n",
        "    \"\"\"Weights update using Gradient descent.\n",
        "    \n",
        "      w' = w - lr * dL/dw\n",
        "    \"\"\"\n",
        "    self.vars = self.vars - self.lr * grads\n",
        "\n",
        "  def history_update(self, z, x, y):\n",
        "    \"\"\"Accumulate all interesting variables\n",
        "    \"\"\"\n",
        "    self.z_history.append(z)\n",
        "    self.x_history.append(x)\n",
        "    self.y_history.append(y)\n",
        "\n",
        "  def train(self, max_steps):\n",
        "    pre_z = 0.0\n",
        "    print(\"steps: {}  z: {:.6f}  x: {:.5f}  y: {:.5f}\".format(0, self.func(self.vars), self.x, self.y))\n",
        "    \n",
        "    file = open('sgd.txt', 'w')\n",
        "    file.write(\"{:.5f}  {:.5f}\\n\".format(self.x, self.y))\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      self.z = self.func(self.vars)\n",
        "      self.history_update(self.z, self.x, self.y)\n",
        "\n",
        "      self.grads = self.gradients(self.vars)\n",
        "      self.weights_update(self.grads)\n",
        "      file.write(\"{:.5f}  {:.5f}\\n\".format(self.x, self.y))\n",
        "    \n",
        "      if (step+1) % 100 == 0:\n",
        "        print(\"steps: {}  z: {:.6f}  x: {:.5f}  y: {:.5f}  dx: {:.5f}  dy: {:.5f}\".format(step+1, self.func(self.vars), self.x, self.y, self.dx, self.dy))\n",
        "      \n",
        "      if np.abs(pre_z - self.z) < 1e-6:\n",
        "        print(\"Enough convergence\")\n",
        "        print(\"steps: {}  z: {:.6f}  x: {:.5f}  y: {:.5f}\".format(step+1, self.func(self.vars), self.x, self.y))\n",
        "        self.z = self.func(self.vars)\n",
        "        self.history_update(self.z, self.x, self.y)\n",
        "        break\n",
        "      \n",
        "      pre_z = self.z\n",
        "    file.close()\n",
        "\n",
        "    self.x_history = np.array(self.x_history)\n",
        "    self.y_history = np.array(self.y_history)\n",
        "    self.path = np.concatenate((np.expand_dims(self.x_history, 1), np.expand_dims(self.y_history, 1)), axis=1).T\n",
        "\n",
        "  @property\n",
        "  def x(self):\n",
        "    return self.vars[0]\n",
        "  \n",
        "  @property\n",
        "  def y(self):\n",
        "    return self.vars[1]\n",
        "  \n",
        "  @property\n",
        "  def dx(self):\n",
        "    return self.grads[0]\n",
        "  \n",
        "  @property\n",
        "  def dy(self):\n",
        "    return self.grads[1]\n",
        "\n",
        "  \n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdi6FEQ-rSMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}